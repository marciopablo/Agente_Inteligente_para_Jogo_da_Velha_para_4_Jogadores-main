# train_final.py
import numpy as np
import time
import os
from environment import TicTacToeEnv
from agent import QAgent
from settings import *

def train_grandmaster():
    print("üëë INICIANDO O TREINO SUPREMO (BATTLE ROYALE) üëë")
    print("Cen√°rio: [2: Elite] | [3: Veterano] | [4: Louco]")
    
    brains_map = {}
    
    if os.path.exists("brain_v2_elite.pkl"):
        print("üíÄ [Assento 2] Mestre Elite: CARREGADO")
        bot_elite = QAgent()
        bot_elite.load_model("brain_v2_elite.pkl")
        brains_map[2] = bot_elite
    else:
        print("‚ö†Ô∏è [Assento 2] Elite n√£o encontrado -> Usando Aleat√≥rio.")

    if os.path.exists("brain.pkl"):
        print("ü§ñ [Assento 3] Veterano: CARREGADO")
        bot_veteran = QAgent()
        bot_veteran.load_model("brain.pkl")
        brains_map[3] = bot_veteran
    else:
        print("‚ö†Ô∏è [Assento 3] Veterano n√£o encontrado -> Usando Aleat√≥rio.")
        
    brains_map[4] = None 

    champion = QAgent()
    
    if os.path.exists("brain_v2_elite.pkl"):
        print("üõ°Ô∏è Campe√£o: Continuando do n√≠vel Elite...")
        champion.load_model("brain_v2_elite.pkl")
    elif os.path.exists("brain.pkl"):
        print("üõ°Ô∏è Campe√£o: Continuando do n√≠vel Veterano...")
        champion.load_model("brain.pkl")
    else:
        print("üê£ Campe√£o: Come√ßando do zero (Vai demorar mais).")
    
    champion.epsilon = 0.2  
    champion.alpha = 0.05   

    env = TicTacToeEnv(opponent_brains=brains_map)
    
    print("-" * 50)
    start_time = time.time()
    recent_wins = [] 
    
    for episode in range(1, EPISODES + 1):
        state = env.reset()
        state_matrix = env.board.copy()
        done = False
        
        while not done:
            valid_moves = [i for i in range(BOARD_SIZE * BOARD_SIZE) if env.is_valid_move(i)]
            if not valid_moves: break

            action = champion.choose_action(state_matrix, valid_moves)
            
            next_state_flat, reward, done, info = env.step(action)
            next_state_matrix = env.board.copy()

            champion.learn(state_matrix, action, reward, next_state_matrix)
            state_matrix = next_state_matrix

        if info.get('result') == 'Win':
            recent_wins.append(1)
        else:
            recent_wins.append(0) 
            
        if len(recent_wins) > 1000: recent_wins.pop(0)

        champion.decay_alpha()
        if champion.epsilon > 0.001:
            champion.epsilon *= 0.99995

        if episode % 1000 == 0:
            win_rate = sum(recent_wins) / len(recent_wins) * 100
            print(f"Epis√≥dio {episode:6d} | Win Rate: {win_rate:5.1f}% | Eps: {champion.epsilon:.3f} | Q-Table: {len(champion.q_table)}")

    total_time = time.time() - start_time
    print("-" * 50)
    print(f"‚úÖ TREINO SUPREMO CONCLU√çDO ({total_time:.1f}s)")
    
    champion.save_model("brain_final_boss.pkl")
    print("üíæ C√âREBRO FINAL SALVO: 'brain_final_boss.pkl'")

if __name__ == "__main__":
    train_grandmaster()